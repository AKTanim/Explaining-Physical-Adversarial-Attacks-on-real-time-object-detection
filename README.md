# Explaining-Physical-Adversarial-Attacks-on-real-time-object-detection
This is my thesis work titled "Explainability of Physical Adversarial Attack against Object Detection as a Path to Robust Defense Technique."

### Abstract
Object detection is a highly active field of computer vision given the growing demand for vision-based systems. In recent years, rapid development and emergence of vision-based systems have been noticed in many real-world applications such as video surveillance, automatic license plate detection, autonomous driving, healthcare, and many other safety-critical domains. However, existing research indicates that object detection models are susceptible to both physical and digital adversarial attacks. This thesis aims to explain why physical adversarial attacks are able to fool real-time object detection models. Explainable AI techniques are utilized to accomplish this purpose. The study also underscores the value of insights derived from the explanations in curating more effective physical adversarial attacks and defenses. Notably, a novel defense method is introduced with partial proof of concept. Moreover, research shows that Transformer-based vision models are more resilient to adversaries than other object detection models, e.g., YOLO, Mobilenet, etc. This study provides a comparative visual explanation of why that is the case. Overall, this work contributes significantly to motivating future breakthroughs in understanding adversarial vulnerabilities of object detection models in real-world applications and developing better defensive techniques as well as more secured object detection models.

P.S. The code is partially available as I have plans for further development and publication.
